behaviors:
    MazeAgent:
        trainer_type: ppo
        hyperparameters:
         # Hyperparameters common to PPO and SAC
            batch_size: 128
            buffer_size: 2048
            learning_rate: 0.0003
            learning_rate_schedule: linear
            
            
              # PPO-specific hyperparameters
            # Replaces the "PPO-specific hyperparameters" section above
            beta: 0.01
            epsilon: 0.2
            lambd: 0.95
            num_epoch: 3
            
            
             # Configuration of the neural network (common to PPO/SAC)
          
        network_settings:
            normalize: false
            hidden_units: 256
            num_layers: 2
            vis_encode_type: simple
            
             # memory
       # memory:
       #     sequence_length: 64
       #     memory_size: 256
        
        
        # Trainer configurations common to all trainers
   # max_steps: 5.0e5
        max_steps: 1000000
        time_horizon: 128
        summary_freq: 500
        keep_checkpoints: 5
        checkpoint_interval: 50000
        threaded: true
     #init_path: null
    
    # behavior cloning
    #   behavioral_cloning:
     #    demo_path: Project/Assets/ML-Agents/Examples/Pyramids/Demos/ExpertPyramid.demo
     #    strength: 0.5
     #    steps: 150000
       #  batch_size: 512
     #    num_epoch: 3
      #   samples_per_update: 0
                 
        reward_signals:
         # environment reward (default) 
            extrinsic:
                gamma: 0.99
                strength: 1.0
                
                
     
        
        